{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tensorflow for Word2Vec. Note that the gensim library is better for the word2vec API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import os\n",
    "import errno\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import tensorflow as tf\n",
    "# import cupy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"word2vec_data/words\"\n",
    "data_url = \"http://mattmahoney.net/dc/text8.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_words_data(url=data_url, words_data=data_dir):\n",
    "    # mkdir if dir doesnt exist\n",
    "    os.makedirs(words_data, exist_ok=True)\n",
    "    # Path to zip file\n",
    "    zip_path = os.path.join(words_data, 'words.zip')\n",
    "    # if !zip file, download it from the data url\n",
    "    if not os.path.exists(zip_path):\n",
    "        print(\"Data not found, downloading from source\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"Download Complete\")\n",
    "    else:\n",
    "        print(\"Data Found\")\n",
    "    # Now the zip file is there, get the data from it\n",
    "    print(\"Reading the data\")\n",
    "    with zipfile.ZipFile(zip_path) as f:\n",
    "        data = f.read(f.namelist()[0])\n",
    "    print(\"Reading Complete\")\n",
    "    # Return a list of all the words in the dataSource\n",
    "    return data.decode('ascii').split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Found\n",
      "Reading the data\n",
      "Reading Complete\n",
      "Data Retrieved\n"
     ]
    }
   ],
   "source": [
    "words = fetch_words_data()\n",
    "print(\"Data Retrieved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 17 Million Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anarchism',\n",
       " 'originated',\n",
       " 'as',\n",
       " 'a',\n",
       " 'term',\n",
       " 'of',\n",
       " 'abuse',\n",
       " 'first',\n",
       " 'used',\n",
       " 'against',\n",
       " 'early',\n",
       " 'working',\n",
       " 'class',\n",
       " 'radicals',\n",
       " 'including',\n",
       " 'the',\n",
       " 'diggers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'english',\n",
       " 'revolution',\n",
       " 'and',\n",
       " 'the',\n",
       " 'sans',\n",
       " 'culottes',\n",
       " 'of',\n",
       " 'the',\n",
       " 'french',\n",
       " 'revolution',\n",
       " 'whilst',\n",
       " 'the',\n",
       " 'term',\n",
       " 'is',\n",
       " 'still',\n",
       " 'used',\n",
       " 'in',\n",
       " 'a',\n",
       " 'pejorative',\n",
       " 'way',\n",
       " 'to',\n",
       " 'describe',\n",
       " 'any',\n",
       " 'act',\n",
       " 'that',\n",
       " 'used',\n",
       " 'violent',\n",
       " 'means',\n",
       " 'to',\n",
       " 'destroy',\n",
       " 'the']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the "
     ]
    }
   ],
   "source": [
    "for word in words[0:50]:\n",
    "    print(word, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the words are already lower cased and the special symbols are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Building a WordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Counter(list) will return a dictionary of the count of the items in the list\n",
    "# Counter(list).most_common(n) will return the count of most common n words in the list\n",
    "\n",
    "def create_counts(vocab_size=50000):\n",
    "    vocab = [] + Counter(words).most_common(vocab_size)\n",
    "    vocab = np.array([word for word,_ in vocab])\n",
    "    dictionary = {word:code for code, word in enumerate(vocab)}\n",
    "    data = np.array([dictionary.get(word,0) for word in words])\n",
    "    return data, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, vocabulary = create_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17005207,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anarchism'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5233"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various words in the data will have an index position in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Batch Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is from Tensorflow doc\n",
    "# When using gensim, all these are taken care of in the background\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    if data_index + span > len(data):\n",
    "        data_index = 0\n",
    "    buffer.extend(data[data_index:data_index + span])\n",
    "    data_index += span\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]\n",
    "            labels[i * num_skips + j, 0] = buffer[target]\n",
    "    if data_index == len(data):\n",
    "        buffer[:] = data[:span]\n",
    "        data_index = span\n",
    "    else:\n",
    "        buffer.append(data[data_index])\n",
    "        data_index += 1\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 150 # Number of Dimension of the word vector\n",
    "skip_window = 1 # How many words to consider on the left and the right of the current word\n",
    "num_skips = 2 # times to resue an input to generate the label\n",
    "valid_size = 16 # Random set of words to evaluate the similarity on\n",
    "valid_window = 100\n",
    "valid_exaples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64\n",
    "learning_rate = 0.01\n",
    "vocabulary_size = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Placeholders, Variables and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/themadscientist/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/themadscientist/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph() # Just in case\n",
    "\n",
    "# Placeholders for the fata\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
    "train_labels = tf.placeholder(tf.int32, shape=[batch_size,1])\n",
    "valid_dataset = tf.constant(valid_exaples, dtype=tf.int32)\n",
    "\n",
    "# Random Initial Embeddings\n",
    "init_embeds = tf.random_uniform(shape=[vocabulary_size, embedding_size], \n",
    "                                minval=-1.0, \n",
    "                                maxval=1.0)\n",
    "# Variables\n",
    "embeddings = tf.Variable(init_embeds)\n",
    "\n",
    "# Looks up ids in a list of embeding tensors\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "# Weights and bias init for noise contrastive estimation (nce)\n",
    "nce_weights = tf.Variable(tf.truncated_normal(shape=[vocabulary_size, embedding_size],\n",
    "                                              stddev=1.0/np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros(shape=[vocabulary_size]))\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, \n",
    "                                     biases=nce_biases, \n",
    "                                     labels=train_labels,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled, \n",
    "                                     num_classes=vocabulary_size))\n",
    "# loss = tf.reduce_mean(\n",
    "#     tf.nn.nce_loss(nce_weights, nce_biases, train_labels, embed,\n",
    "#                    num_sampled, vocabulary_size))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1.0) # Okay, lets see\n",
    "trainer = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity between MiniBatchExamples and all Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-02a02e7006c1>:1: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), axis=1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Variables Initializer Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on GPU\n",
    "# gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step  0  is  269.75390625\n",
      "Average loss at step  1000  is  10571.551780364991\n",
      "Average loss at step  2000  is  23219.775340682983\n",
      "Average loss at step  3000  is  26599.585041854858\n",
      "Average loss at step  4000  is  30621.27516558838\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_steps = 5000 # usually do it for 200000 steps but my sweet Nvidia GTX 1060 is at home\n",
    "\n",
    "# with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "with tf.Session() as sess:\n",
    "    # Initializing Variables\n",
    "    sess.run(init)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs,\n",
    "                     train_labels: batch_labels}\n",
    "        _, loss_val = sess.run([trainer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        average_loss += loss_val\n",
    "        \n",
    "        if step %100 == 0:\n",
    "            if step >0:\n",
    "                average_loss /=  1000\n",
    "            print(\"Average loss at step \", step, \" is \", average_loss)\n",
    "            average_loss = 0\n",
    "        final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35329.761723060605"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_loss/1000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
